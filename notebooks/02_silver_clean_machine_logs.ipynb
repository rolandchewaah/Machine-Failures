{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when, coalesce, lit\n",
    "\n",
    "# Load Bronze from Unity Catalog (update catalog/schema as needed)\n",
    "df_bronze = spark.read.format(\"delta\").table(\"main.default.bronze_machine_failure\")\n",
    "\n",
    "print(f\"Bronze records: {df_bronze.count()}\")\n",
    "\n",
    "# Clean and transform\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"is_failure\", \n",
    "                when(col(\"error_code\").isNotNull() & (col(\"error_code\") != \"\"), 1)\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"temperature\", col(\"temperature\").cast(\"double\")) \\\n",
    "    .filter(col(\"machine_id\").isNotNull() & col(\"timestamp\").isNotNull())\n",
    "\n",
    "# Show data quality metrics\n",
    "print(f\"Silver records: {df_silver.count()}\")\n",
    "print(f\"Failures detected: {df_silver.filter(col('is_failure') == 1).count()}\")\n",
    "display(df_silver.limit(10))\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.default.silver_machine_failure_clean\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
