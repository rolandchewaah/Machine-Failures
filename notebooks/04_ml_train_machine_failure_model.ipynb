{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Enable MLflow autologging\n",
    "mlflow.spark.autolog()\n",
    "\n",
    "# Load Gold from Unity Catalog\n",
    "df_gold = spark.read.format(\"delta\").table(\"main.default.gold_machine_features\")\n",
    "\n",
    "# Select features and label\n",
    "features = [\"temperature\", \"pressure\", \"vibration\", \"rpm\", \n",
    "            \"temp_roll_avg\", \"temp_diff\", \"pressure_roll_avg\", \"vibration_roll_avg\"]\n",
    "\n",
    "df_ml = df_gold.select(\"is_failure\", *features).na.drop()\n",
    "\n",
    "print(f\"Total records: {df_ml.count()}\")\n",
    "print(f\"Failure cases: {df_ml.filter('is_failure = 1').count()}\")\n",
    "print(f\"Non-failure cases: {df_ml.filter('is_failure = 0').count()}\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Classifier with explicit parameters\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"is_failure\", \n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train/test split\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set: {train.count()}, Test set: {test.count()}\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"machine_failure_rf\") as run:\n",
    "    # Train\n",
    "    model = pipeline.fit(train)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    # Evaluate with multiple metrics\n",
    "    auc_evaluator = BinaryClassificationEvaluator(labelCol=\"is_failure\", metricName=\"areaUnderROC\")\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_failure\", metricName=\"accuracy\")\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    \n",
    "    f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_failure\", metricName=\"f1\")\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Log additional metrics\n",
    "    mlflow.log_metric(\"test_auc\", auc)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"test_f1\", f1)\n",
    "    \n",
    "    # Display sample predictions\n",
    "    display(predictions.select(\"is_failure\", \"prediction\", \"probability\", *features[:3]).limit(10))\n",
    "\n",
    "print(f\"\\nMLflow Run ID: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
